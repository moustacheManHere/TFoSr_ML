{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load the data\n",
    "train_df = pd.read_csv('data/train.csv')\n",
    "val_df = pd.read_csv('data/val.csv')\n",
    "\n",
    "# Prepare features and targets\n",
    "X_train = train_df.drop(columns=['class']).values\n",
    "y_train = train_df['class'].values\n",
    "X_val = val_df.drop(columns=['class']).values\n",
    "y_val = val_df['class'].values\n",
    "\n",
    "# Step 1: One-hot encode the labels\n",
    "onehot_encoder = OneHotEncoder(sparse_output=False)\n",
    "y_train_encoded = onehot_encoder.fit_transform(y_train.reshape(-1, 1))\n",
    "y_val_encoded = onehot_encoder.transform(y_val.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the data into PyTorch tensors\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train_encoded, dtype=torch.float32)\n",
    "X_val_tensor = torch.tensor(X_val, dtype=torch.float32)\n",
    "y_val_tensor = torch.tensor(y_val_encoded, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Define the MLP model\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(MLP, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.fc3 = nn.Linear(hidden_size, output_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.relu(self.fc2(x))\n",
    "        x = self.fc3(x)  # No activation on the last layer since we'll use CrossEntropyLoss\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model parameters\n",
    "input_size = X_train.shape[1]  # Number of keypoints (42 features, 21 x, y coordinates)\n",
    "hidden_size = 128  # You can adjust this\n",
    "output_size = y_train_encoded.shape[1]  # Number of classes\n",
    "\n",
    "# Step 3: Initialize the model, loss function, and optimizer\n",
    "model = MLP(input_size, hidden_size, output_size)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, X_train_tensor, y_train_tensor, epochs=100):\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        # Zero the gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(X_train_tensor)\n",
    "        \n",
    "        # Compute the loss\n",
    "        loss = criterion(outputs, torch.max(y_train_tensor, 1)[1])  # Use max to get class indices\n",
    "        \n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if (epoch+1) % 10 == 0:\n",
    "            print(f'Epoch [{epoch+1}/{epochs}], Loss: {loss.item():.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/2000], Loss: 0.2076\n",
      "Epoch [20/2000], Loss: 0.2057\n",
      "Epoch [30/2000], Loss: 0.2028\n",
      "Epoch [40/2000], Loss: 0.2008\n",
      "Epoch [50/2000], Loss: 0.1984\n",
      "Epoch [60/2000], Loss: 0.1958\n",
      "Epoch [70/2000], Loss: 0.1939\n",
      "Epoch [80/2000], Loss: 0.1914\n",
      "Epoch [90/2000], Loss: 0.1888\n",
      "Epoch [100/2000], Loss: 0.1868\n",
      "Epoch [110/2000], Loss: 0.1849\n",
      "Epoch [120/2000], Loss: 0.1828\n",
      "Epoch [130/2000], Loss: 0.1802\n",
      "Epoch [140/2000], Loss: 0.1779\n",
      "Epoch [150/2000], Loss: 0.1757\n",
      "Epoch [160/2000], Loss: 0.1737\n",
      "Epoch [170/2000], Loss: 0.1714\n",
      "Epoch [180/2000], Loss: 0.1693\n",
      "Epoch [190/2000], Loss: 0.1668\n",
      "Epoch [200/2000], Loss: 0.1645\n",
      "Epoch [210/2000], Loss: 0.1625\n",
      "Epoch [220/2000], Loss: 0.1601\n",
      "Epoch [230/2000], Loss: 0.1578\n",
      "Epoch [240/2000], Loss: 0.1563\n",
      "Epoch [250/2000], Loss: 0.1537\n",
      "Epoch [260/2000], Loss: 0.1515\n",
      "Epoch [270/2000], Loss: 0.1495\n",
      "Epoch [280/2000], Loss: 0.1478\n",
      "Epoch [290/2000], Loss: 0.1459\n",
      "Epoch [300/2000], Loss: 0.1440\n",
      "Epoch [310/2000], Loss: 0.1419\n",
      "Epoch [320/2000], Loss: 0.1401\n",
      "Epoch [330/2000], Loss: 0.1384\n",
      "Epoch [340/2000], Loss: 0.1364\n",
      "Epoch [350/2000], Loss: 0.1345\n",
      "Epoch [360/2000], Loss: 0.1330\n",
      "Epoch [370/2000], Loss: 0.1312\n",
      "Epoch [380/2000], Loss: 0.1299\n",
      "Epoch [390/2000], Loss: 0.1282\n",
      "Epoch [400/2000], Loss: 0.1264\n",
      "Epoch [410/2000], Loss: 0.1247\n",
      "Epoch [420/2000], Loss: 0.1235\n",
      "Epoch [430/2000], Loss: 0.1219\n",
      "Epoch [440/2000], Loss: 0.1202\n",
      "Epoch [450/2000], Loss: 0.1187\n",
      "Epoch [460/2000], Loss: 0.1172\n",
      "Epoch [470/2000], Loss: 0.1159\n",
      "Epoch [480/2000], Loss: 0.1148\n",
      "Epoch [490/2000], Loss: 0.1131\n",
      "Epoch [500/2000], Loss: 0.1117\n",
      "Epoch [510/2000], Loss: 0.1104\n",
      "Epoch [520/2000], Loss: 0.1092\n",
      "Epoch [530/2000], Loss: 0.1080\n",
      "Epoch [540/2000], Loss: 0.1067\n",
      "Epoch [550/2000], Loss: 0.1054\n",
      "Epoch [560/2000], Loss: 0.1040\n",
      "Epoch [570/2000], Loss: 0.1033\n",
      "Epoch [580/2000], Loss: 0.1016\n",
      "Epoch [590/2000], Loss: 0.1005\n",
      "Epoch [600/2000], Loss: 0.0994\n",
      "Epoch [610/2000], Loss: 0.0981\n",
      "Epoch [620/2000], Loss: 0.0967\n",
      "Epoch [630/2000], Loss: 0.0959\n",
      "Epoch [640/2000], Loss: 0.0949\n",
      "Epoch [650/2000], Loss: 0.0936\n",
      "Epoch [660/2000], Loss: 0.0921\n",
      "Epoch [670/2000], Loss: 0.0910\n",
      "Epoch [680/2000], Loss: 0.0898\n",
      "Epoch [690/2000], Loss: 0.0888\n",
      "Epoch [700/2000], Loss: 0.0880\n",
      "Epoch [710/2000], Loss: 0.0870\n",
      "Epoch [720/2000], Loss: 0.0858\n",
      "Epoch [730/2000], Loss: 0.0850\n",
      "Epoch [740/2000], Loss: 0.0838\n",
      "Epoch [750/2000], Loss: 0.0827\n",
      "Epoch [760/2000], Loss: 0.0819\n",
      "Epoch [770/2000], Loss: 0.0810\n",
      "Epoch [780/2000], Loss: 0.0799\n",
      "Epoch [790/2000], Loss: 0.0795\n",
      "Epoch [800/2000], Loss: 0.0780\n",
      "Epoch [810/2000], Loss: 0.0773\n",
      "Epoch [820/2000], Loss: 0.0763\n",
      "Epoch [830/2000], Loss: 0.0757\n",
      "Epoch [840/2000], Loss: 0.0750\n",
      "Epoch [850/2000], Loss: 0.0742\n",
      "Epoch [860/2000], Loss: 0.0733\n",
      "Epoch [870/2000], Loss: 0.0723\n",
      "Epoch [880/2000], Loss: 0.0714\n",
      "Epoch [890/2000], Loss: 0.0704\n",
      "Epoch [900/2000], Loss: 0.0699\n",
      "Epoch [910/2000], Loss: 0.0689\n",
      "Epoch [920/2000], Loss: 0.0681\n",
      "Epoch [930/2000], Loss: 0.0672\n",
      "Epoch [940/2000], Loss: 0.0667\n",
      "Epoch [950/2000], Loss: 0.0659\n",
      "Epoch [960/2000], Loss: 0.0651\n",
      "Epoch [970/2000], Loss: 0.0644\n",
      "Epoch [980/2000], Loss: 0.0638\n",
      "Epoch [990/2000], Loss: 0.0631\n",
      "Epoch [1000/2000], Loss: 0.0623\n",
      "Epoch [1010/2000], Loss: 0.0617\n",
      "Epoch [1020/2000], Loss: 0.0609\n",
      "Epoch [1030/2000], Loss: 0.0603\n",
      "Epoch [1040/2000], Loss: 0.0596\n",
      "Epoch [1050/2000], Loss: 0.0590\n",
      "Epoch [1060/2000], Loss: 0.0582\n",
      "Epoch [1070/2000], Loss: 0.0575\n",
      "Epoch [1080/2000], Loss: 0.0568\n",
      "Epoch [1090/2000], Loss: 0.0563\n",
      "Epoch [1100/2000], Loss: 0.0558\n",
      "Epoch [1110/2000], Loss: 0.0553\n",
      "Epoch [1120/2000], Loss: 0.0548\n",
      "Epoch [1130/2000], Loss: 0.0538\n",
      "Epoch [1140/2000], Loss: 0.0533\n",
      "Epoch [1150/2000], Loss: 0.0529\n",
      "Epoch [1160/2000], Loss: 0.0521\n",
      "Epoch [1170/2000], Loss: 0.0519\n",
      "Epoch [1180/2000], Loss: 0.0512\n",
      "Epoch [1190/2000], Loss: 0.0504\n",
      "Epoch [1200/2000], Loss: 0.0499\n",
      "Epoch [1210/2000], Loss: 0.0495\n",
      "Epoch [1220/2000], Loss: 0.0488\n",
      "Epoch [1230/2000], Loss: 0.0484\n",
      "Epoch [1240/2000], Loss: 0.0479\n",
      "Epoch [1250/2000], Loss: 0.0473\n",
      "Epoch [1260/2000], Loss: 0.0472\n",
      "Epoch [1270/2000], Loss: 0.0467\n",
      "Epoch [1280/2000], Loss: 0.0459\n",
      "Epoch [1290/2000], Loss: 0.0454\n",
      "Epoch [1300/2000], Loss: 0.0448\n",
      "Epoch [1310/2000], Loss: 0.0446\n",
      "Epoch [1320/2000], Loss: 0.0439\n",
      "Epoch [1330/2000], Loss: 0.0437\n",
      "Epoch [1340/2000], Loss: 0.0431\n",
      "Epoch [1350/2000], Loss: 0.0426\n",
      "Epoch [1360/2000], Loss: 0.0421\n",
      "Epoch [1370/2000], Loss: 0.0419\n",
      "Epoch [1380/2000], Loss: 0.0417\n",
      "Epoch [1390/2000], Loss: 0.0410\n",
      "Epoch [1400/2000], Loss: 0.0403\n",
      "Epoch [1410/2000], Loss: 0.0400\n",
      "Epoch [1420/2000], Loss: 0.0397\n",
      "Epoch [1430/2000], Loss: 0.0394\n",
      "Epoch [1440/2000], Loss: 0.0387\n",
      "Epoch [1450/2000], Loss: 0.0385\n",
      "Epoch [1460/2000], Loss: 0.0382\n",
      "Epoch [1470/2000], Loss: 0.0376\n",
      "Epoch [1480/2000], Loss: 0.0372\n",
      "Epoch [1490/2000], Loss: 0.0371\n",
      "Epoch [1500/2000], Loss: 0.0366\n",
      "Epoch [1510/2000], Loss: 0.0361\n",
      "Epoch [1520/2000], Loss: 0.0357\n",
      "Epoch [1530/2000], Loss: 0.0353\n",
      "Epoch [1540/2000], Loss: 0.0351\n",
      "Epoch [1550/2000], Loss: 0.0349\n",
      "Epoch [1560/2000], Loss: 0.0345\n",
      "Epoch [1570/2000], Loss: 0.0341\n",
      "Epoch [1580/2000], Loss: 0.0335\n",
      "Epoch [1590/2000], Loss: 0.0335\n",
      "Epoch [1600/2000], Loss: 0.0329\n",
      "Epoch [1610/2000], Loss: 0.0326\n",
      "Epoch [1620/2000], Loss: 0.0322\n",
      "Epoch [1630/2000], Loss: 0.0319\n",
      "Epoch [1640/2000], Loss: 0.0317\n",
      "Epoch [1650/2000], Loss: 0.0315\n",
      "Epoch [1660/2000], Loss: 0.0311\n",
      "Epoch [1670/2000], Loss: 0.0308\n",
      "Epoch [1680/2000], Loss: 0.0305\n",
      "Epoch [1690/2000], Loss: 0.0303\n",
      "Epoch [1700/2000], Loss: 0.0299\n",
      "Epoch [1710/2000], Loss: 0.0295\n",
      "Epoch [1720/2000], Loss: 0.0292\n",
      "Epoch [1730/2000], Loss: 0.0290\n",
      "Epoch [1740/2000], Loss: 0.0286\n",
      "Epoch [1750/2000], Loss: 0.0285\n",
      "Epoch [1760/2000], Loss: 0.0282\n",
      "Epoch [1770/2000], Loss: 0.0279\n",
      "Epoch [1780/2000], Loss: 0.0276\n",
      "Epoch [1790/2000], Loss: 0.0273\n",
      "Epoch [1800/2000], Loss: 0.0272\n",
      "Epoch [1810/2000], Loss: 0.0270\n",
      "Epoch [1820/2000], Loss: 0.0268\n",
      "Epoch [1830/2000], Loss: 0.0265\n",
      "Epoch [1840/2000], Loss: 0.0262\n",
      "Epoch [1850/2000], Loss: 0.0260\n",
      "Epoch [1860/2000], Loss: 0.0258\n",
      "Epoch [1870/2000], Loss: 0.0255\n",
      "Epoch [1880/2000], Loss: 0.0252\n",
      "Epoch [1890/2000], Loss: 0.0250\n",
      "Epoch [1900/2000], Loss: 0.0248\n",
      "Epoch [1910/2000], Loss: 0.0245\n",
      "Epoch [1920/2000], Loss: 0.0243\n",
      "Epoch [1930/2000], Loss: 0.0240\n",
      "Epoch [1940/2000], Loss: 0.0239\n",
      "Epoch [1950/2000], Loss: 0.0237\n",
      "Epoch [1960/2000], Loss: 0.0235\n",
      "Epoch [1970/2000], Loss: 0.0233\n",
      "Epoch [1980/2000], Loss: 0.0231\n",
      "Epoch [1990/2000], Loss: 0.0228\n",
      "Epoch [2000/2000], Loss: 0.0227\n"
     ]
    }
   ],
   "source": [
    "train(model, X_train_tensor, y_train_tensor, epochs=2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, X_val_tensor, y_val_tensor):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        outputs = model(X_val_tensor)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        _, labels = torch.max(y_val_tensor, 1)\n",
    "        accuracy = accuracy_score(predicted.cpu(), labels.cpu())\n",
    "    return accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.92\n"
     ]
    }
   ],
   "source": [
    "accuracy = evaluate(model, X_val_tensor, y_val_tensor)\n",
    "print(f'Validation Accuracy: {accuracy:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'mlp_hand_sign_classifier.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loaded_model = MLP(42, 128, 26)\n",
    "\n",
    "loaded_model.load_state_dict(torch.load('mlp_hand_sign_classifier.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy of the loaded model: 0.92\n"
     ]
    }
   ],
   "source": [
    "loaded_model.eval()\n",
    "\n",
    "accuracy = evaluate(loaded_model, X_val_tensor, y_val_tensor)\n",
    "print(f'Validation Accuracy of the loaded model: {accuracy:.2f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "newml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
